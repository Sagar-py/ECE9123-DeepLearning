{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1prob4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ727lB7Bd-Q"
      },
      "source": [
        "OK, thus far we have been talking about linear models. All these can be viewed as a single-layer neural net. The next step is to move on to multi-layer nets. Training these is a bit more involved, and implementing from scratch requires time and effort. Instead, we just use well-established libraries. I prefer PyTorch, which is based on an earlier library called Torch (designed for training neural nets via backprop)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zHrM6ihB1uA"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5vzZmhnCBDv"
      },
      "source": [
        "Torch handles data types a bit differently. Everything in torch is a *tensor*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvAolsJ6CJJs"
      },
      "source": [
        "a = np.random.rand(2,3)\n",
        "b = torch.from_numpy(a)\n",
        "\n",
        "# Q4.1 Display the contents of a, b\n",
        "# ...\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuRIMzC-CYx-"
      },
      "source": [
        "The idea in Torch is that tensors allow for easy forward (function evaluations) and backward (gradient) passes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb2LWPSACqiS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9f0310a0-52a5-428d-9341-ce1bdda374ec"
      },
      "source": [
        "A = torch.rand(2,2)\n",
        "b = torch.rand(2,1)\n",
        "x = torch.rand(2,1, requires_grad=True)\n",
        "\n",
        "y = torch.matmul(A,x) + b\n",
        "\n",
        "print(y)\n",
        "z = y.sum()\n",
        "print(z)\n",
        "z.backward()\n",
        "print(x.grad)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.5319],\n",
            "        [1.9999]], grad_fn=<AddBackward0>)\n",
            "tensor(3.5318, grad_fn=<SumBackward0>)\n",
            "tensor([[1.3173],\n",
            "        [1.7640]])\n",
            "tensor([[0.6014],\n",
            "        [0.6757]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ptw5BlEHVL"
      },
      "source": [
        "Notice how the backward pass computed the gradients using autograd. OK, enough background. Time to train some networks. Let us load the *Fashion MNIST* dataset, which is a database of grayscale images of clothing items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSTq5wKoEPlA"
      },
      "source": [
        "trainingdata = torchvision.datasets.FashionMNIST('./FashionMNIST/',train=True,download=True,transform=torchvision.transforms.ToTensor())\n",
        "testdata = torchvision.datasets.FashionMNIST('./FashionMNIST/',train=False,download=True,transform=torchvision.transforms.ToTensor())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_viGcCfmMxJp"
      },
      "source": [
        "Let us examine the size of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMb0WQthE2vR"
      },
      "source": [
        "# Q4.2 How many training and testing data points are there in the dataset? \n",
        "# What is the number of features in each data point?\n",
        "# ...\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beaahX9xFQf2"
      },
      "source": [
        "Let us try to visualize some of the images. Since each data point is a tensor (not an array) we need to postprocess to use matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaEVjo9BFYQo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "image, label = trainingdata[0]\n",
        "# Q4.3 Assuming each sample is an image of size 28x28, show it in matplotlib.\n",
        "# ...\n",
        "# ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Yg31snFlNa"
      },
      "source": [
        "Let's try plotting several images. This is conveniently achieved in PyTorch using a *data loader*, which loads data in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rypRoXGKHG2a"
      },
      "source": [
        "trainDataLoader = torch.utils.data.DataLoader(trainingdata, batch_size=64, shuffle=True)\n",
        "testDataLoader = torch.utils.data.DataLoader(testdata, batch_size=64, shuffle=False)\n",
        "images, labels = iter(trainDataLoader).next()\n",
        "print(images.size(), labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq0HcYLCIq1B"
      },
      "source": [
        "# Q4.4 Visualize the first 10 images of the first minibatch \n",
        "# returned by testDataLoader.\n",
        "# ...\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWC-g9OTOPAq"
      },
      "source": [
        "Now we are ready to define our linear model. Here is some boilerplate PyTorch code that implements the forward model for a single layer network for logistic regression (similar to the one discussed in class notes). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUI2KGrDJpRx"
      },
      "source": [
        "class LinearReg(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LinearReg, self).__init__()\n",
        "    self.linear = torch.nn.Linear(28*28,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1,28*28)\n",
        "    transformed_x = self.linear(x)\n",
        "    return transformed_x\n",
        "\n",
        "net = LinearReg().cuda()\n",
        "Loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfCQuKX7LAqV"
      },
      "source": [
        "Cool! Now we have set everything up. Let's try to train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72TiXpC8LEMX"
      },
      "source": [
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "# Q4.5 Write down a for-loop that trains this network for 20 epochs, \n",
        "# and print the train/test losses.\n",
        "# Save them in the variables above. If done correctly, you should be able to \n",
        "# execute the next code block.\n",
        "# ...\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pY0PZU2NWWf"
      },
      "source": [
        "plt.plot(range(20),train_loss_history,'-',linewidth=3,label='Train error')\n",
        "plt.plot(range(20),test_loss_history,'-',linewidth=3,label='Test error')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e1AOrb7PCQ2"
      },
      "source": [
        "Neat! Now let's evaluate our model accuracy on the entire dataset. The predicted class label for a given input image can computed by looking at the output of the neural network model and computing the index corresponding to the maximum activation. Something like\n",
        "\n",
        "*predicted_output = net(images)*\n",
        "*_, predicted_labels = torch.max(predicted_output,1)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpbhEoBGPDs6"
      },
      "source": [
        "predicted_output = net(images)\n",
        "print(torch.max(predicted_output, 1))\n",
        "fit = Loss(predicted_output, labels)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lpCJphqiyKU"
      },
      "source": [
        "def evaluate(dataloader):\n",
        "  # Q4.6 Implement a function here that evaluates training and testing accuracy.\n",
        "  # Here, accuracy is measured by probability of successful classification.\n",
        "  # ...\n",
        "  # ...\n",
        "\n",
        "print('Train acc = %0.2f, test acc = %0.2f' % (evaluate(trainDataLoader), evaluate(testDataLoader)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}