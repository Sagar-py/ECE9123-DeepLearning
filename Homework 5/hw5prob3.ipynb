{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw6-ttt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdLtY9WviBTN"
      },
      "source": [
        "In this exercise we will train a simple Q-network in TensorFlow to solve Tic Tac Toe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z29dkm2oA4_"
      },
      "source": [
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b396uo-RiNyl"
      },
      "source": [
        "Hopefully everyone has played Tic Tac Toe at some point. Here is a [reminder](https://en.wikipedia.org/wiki/Tic-tac-toe). Let us set up some helper functions to define the game itself. The typical board size is 3x3 but we will be general."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv_30ZVSoA4_"
      },
      "source": [
        "def new_board(size):\n",
        "    return np.zeros(shape=(size, size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxSWzNXLoA4_"
      },
      "source": [
        "def available_moves(board):\n",
        "    return np.argwhere(board == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPIz-ZocoA4_"
      },
      "source": [
        "def check_game_end(board):\n",
        "    best = max(list(board.sum(axis=0)) +    # columns\n",
        "               list(board.sum(axis=1)) +    # rows\n",
        "               [board.trace()] +            # main diagonal\n",
        "               [np.fliplr(board).trace()],  # other diagonal\n",
        "               key=abs)\n",
        "    if abs(best) == board.shape[0]: \n",
        "        return np.sign(best)  # winning player, +1 or -1\n",
        "    if available_moves(board).size == 0:\n",
        "        return 0  # a draw (otherwise, return None by default)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV0_JWY_iz0U"
      },
      "source": [
        "Now, let's define our players. We will define three types of bots. A *random* player picks a random position in the board each move."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg7huG59oA4_"
      },
      "source": [
        "class Player():\n",
        "    def new_game(self):\n",
        "        pass\n",
        "    def reward(self, value):\n",
        "        pass\n",
        "\n",
        "class RandomPlayer(Player):\n",
        "    def move(self, board):\n",
        "        return random.choice(available_moves(board))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfNmM7sHjIoq"
      },
      "source": [
        "A *boring* player always picks the *first* available position on the board (measured from top-left to bottom-right)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH3F96S0oA4_"
      },
      "source": [
        "class BoringPlayer(Player):\n",
        "    def move(self, board):\n",
        "        return available_moves(board)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oUX34hMj1aj"
      },
      "source": [
        "We can simulate games by playing one bot vs another. The starting player is labeled +1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLcrjxQioA4_"
      },
      "source": [
        "def play(board, player_objs):\n",
        "    for player in [+1, -1]:\n",
        "        player_objs[player].new_game()\n",
        "    player = +1\n",
        "    game_end = check_game_end(board)\n",
        "    while game_end is None:\n",
        "        move = player_objs[player].move(board)\n",
        "        board[tuple(move)] = player\n",
        "        game_end = check_game_end(board)\n",
        "        player *= -1  # switch players after each move\n",
        "    for player in [+1, -1]:\n",
        "        # the reward for wins is +1, and -1 for draws/losses\n",
        "        reward_value = +1 if player == game_end else -1\n",
        "        player_objs[player].reward(reward_value)\n",
        "    return game_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-InFBCMooA4_"
      },
      "source": [
        "# 3x3, random vs. random\n",
        "random.seed(1)\n",
        "\n",
        "# TODO Q1. Play 2000 games between two bots, both of them random players.\n",
        "# Print the number of wins by Player 1, number of wins by Player 2, and draws.\n",
        "# Plot (as a function of game index) the moving average of game outcomes\n",
        "# over a window of size 500. \n",
        "# You might find the following functions helpful for plotting.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdAMumwhoA4_"
      },
      "source": [
        "def moving(data, value=+1, size):# calculates a moving average\n",
        "    binary_data = [x == value for x in data]\n",
        "    return [sum(binary_data[i-size:i])/size for i in range(size, len(data) + 1)]\n",
        "\n",
        "def show(results, size=500, title='Moving average of game outcomes',\n",
        "         first_label='First Player Wins', second_label='Second Player Wins', draw_label='Draw'):\n",
        "    x_values = range(size, len(results) + 1)\n",
        "    first = moving(results, value=+1, size=size)\n",
        "    second = moving(results, value=-1, size=size)\n",
        "    draw = moving(results, value=0, size=size)\n",
        "    first, = plt.plot(x_values, first, color='red', label=first_label)\n",
        "    second, = plt.plot(x_values, second, color='blue', label=second_label)\n",
        "    draw, = plt.plot(x_values, draw, color='grey', label=draw_label)\n",
        "    plt.xlim([0, len(results)])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.title(title)\n",
        "    plt.legend(handles=[first, second, draw], loc='best')\n",
        "    ax = plt.gca()\n",
        "    ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax=1))\n",
        "    ax.xaxis.set_major_formatter(matplotlib.ticker.StrMethodFormatter('{x:,.0f}'))\n",
        "    plt.ylabel(f'Rate over last {size} games')\n",
        "    plt.xlabel('Game index')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSTphX8_oA4_"
      },
      "source": [
        "# 3x3, random vs. boring\n",
        "\n",
        "# TODO Q2. Play 2000 games between two bots, where Player 1 is Random and Player 2 is Boring.\n",
        "# Print the number of wins by Player 1, number of wins by Player 2, and draws.\n",
        "# Plot (as a function of game index) the moving average of game outcomes.\n",
        "\n",
        "# Comment on the results. Compare with your plot above. \n",
        "#Think about why this might be happening and explain your reasons."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRwAxMN8l1fJ"
      },
      "source": [
        "We will now use Q-learning using a neural network to train an RL agent. \n",
        "\n",
        "The Q-function will be parametrically represented via a very simple single layer with linear activations (essentially, a linear model).\n",
        "\n",
        "Complete the Q-learning part in the code snippet below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PSqOYqVoA5A"
      },
      "source": [
        "class Agent(Player):\n",
        "\n",
        "    # Define single layer Q-network, MSE loss, and SGD optimizer\n",
        "    def __init__(self, size, seed):\n",
        "        self.size = size\n",
        "        self.training = True\n",
        "        self.model = tf.keras.Sequential()\n",
        "        self.model.add(tf.keras.layers.Dense(\n",
        "            size**2,\n",
        "            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=seed)))\n",
        "        self.model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "    # Helper function to predict the Q-function\n",
        "    def predict_q(self, board):\n",
        "        return self.model.predict(\n",
        "            np.array([board.ravel()])).reshape(self.size, self.size)\n",
        "\n",
        "    # Helper function to train the network\n",
        "    def fit_q(self, board, q_values):\n",
        "        self.model.fit(\n",
        "            np.array([board.ravel()]), np.array([q_values.ravel()]), verbose=0)\n",
        "\n",
        "    # The agent preserves history, which is reset when a new game starts.\n",
        "    def new_game(self):\n",
        "        self.last_move = None\n",
        "        self.board_history = []\n",
        "        self.q_history = []\n",
        "\n",
        "    # TODO Q3: Implement the \"move\" method below.\n",
        "    # The \"move\" method should use the output of the Q-network \n",
        "    # that you defined above to pick the next best move.\n",
        "    # Make sure you are only picking \"legal\" moves. \n",
        "\n",
        "    def move(self, board):\n",
        "        # ... COMPLETE THIS\n",
        "\n",
        "        if self.last_move is not None:\n",
        "            self.reward(value)\n",
        "        self.board_history.append(board.copy())\n",
        "        self.q_history.append(q_values)\n",
        "        return move\n",
        "\n",
        "    # After picking the move, we call the reward method.\n",
        "    # The reward method trains the Q-network, updating the Q-values with \n",
        "    # a new estimate for the last move. This is the Bellman update.\n",
        "    def reward(self, reward_value):\n",
        "        if not self.training:\n",
        "            return\n",
        "        new_q = self.q_history[-1].copy()\n",
        "        new_q[self.last_move] = reward_value\n",
        "        self.fit_q(self.board_history[-1], new_q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQc7ENOnoA5A"
      },
      "source": [
        "# 3x3, q-learning vs. random\n",
        "\n",
        "# TODO Q4. Play 2000 games, where Player 1 is a Q-network and Player 2 is Random.\n",
        "# Print the number of wins by Player 1, number of wins by Player 2, and draws.\n",
        "# Plot (as a function of game index) the moving average of game outcomes.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}